{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "from nltk.tokenize import LineTokenizer\n",
    "import ngram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = tfds.load(name='tiny_shakespeare')['train']\n",
    "# decode the text\n",
    "text = [x['text'].numpy() for x in list(d)]\n",
    "\n",
    "# need to remove the b' from all data\n",
    "text = [x.decode('utf-8') for x in text][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['<s>',\n",
       "  '<s>',\n",
       "  'First',\n",
       "  'Citizen',\n",
       "  ':',\n",
       "  'Before',\n",
       "  'we',\n",
       "  'proceed',\n",
       "  'any',\n",
       "  'further',\n",
       "  ',',\n",
       "  'hear',\n",
       "  'me',\n",
       "  'speak',\n",
       "  '.',\n",
       "  '</s>',\n",
       "  '</s>']]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = ngram.tokenize(text, 3)\n",
    "held_out_corpus = sentences[:1]\n",
    "\n",
    "held_out_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split by sentences but be smart about it\n",
    "mle = ngram.MLE(3, np.concatenate(held_out_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({('<s>', '<s>', 'First'): 1,\n",
       "         ('<s>', 'First', 'Citizen'): 1,\n",
       "         ('First', 'Citizen', ':'): 1,\n",
       "         ('Citizen', ':', 'Before'): 1,\n",
       "         (':', 'Before', 'we'): 1,\n",
       "         ('Before', 'we', 'proceed'): 1,\n",
       "         ('we', 'proceed', 'any'): 1,\n",
       "         ('proceed', 'any', 'further'): 1,\n",
       "         ('any', 'further', ','): 1,\n",
       "         ('further', ',', 'hear'): 1,\n",
       "         (',', 'hear', 'me'): 1,\n",
       "         ('hear', 'me', 'speak'): 1,\n",
       "         ('me', 'speak', '.'): 1,\n",
       "         ('speak', '.', '</s>'): 1,\n",
       "         ('.', '</s>', '</s>'): 1})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = Tokenizer(char_level=False)\n",
    "texts = [text.numpy().decode('utf-8').strip() for text in d.map(lambda x: x['text'])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_word_occurrences = sum(tok.word_counts.values())\n",
    "vocab_size = len(tok.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_word_occurrences, vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

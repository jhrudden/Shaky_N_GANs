{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johnhenryrudden/anaconda3/envs/shaky_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/johnhenryrudden/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from embeddings import SentenceEmbeddingDataset, WordEmbeddingManager, create_embedding_dataloader\n",
    "import embeddings\n",
    "import utils\n",
    "from gan import Generator, Discriminator\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = 'data/raw_train.txt'\n",
    "tokenized_sentences = utils.process_training_data(TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longest sentence has 20 tokens\n"
     ]
    }
   ],
   "source": [
    "longest_sentence = max(tokenized_sentences, key=len)\n",
    "print(f'Longest sentence has {len(longest_sentence)} tokens')\n",
    "SEQ_LENGTH = len(longest_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully from data/word2vec.model\n"
     ]
    }
   ],
   "source": [
    "WORD2VEC_MODEL_PATH = 'data/word2vec.model'\n",
    "word2vec_manager = WordEmbeddingManager(WORD2VEC_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = create_embedding_dataloader(tokenized_sentences, word2vec_manager, seq_length=SEQ_LENGTH, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 7.1316e-03,  1.9339e-02, -2.7511e-02,  6.9083e-03, -1.6032e-02,\n",
       "         -4.1205e-02,  6.9458e-02,  1.0333e-01, -8.4358e-02,  1.8224e-03,\n",
       "         -3.7226e-02, -3.8608e-02, -2.3026e-02,  5.3395e-02, -7.1094e-02,\n",
       "          5.7522e-03,  2.3071e-02, -3.9609e-02, -9.7750e-02, -2.4183e-03,\n",
       "          1.7771e-02,  2.1389e-02,  4.9973e-02, -1.1299e-02,  3.1689e-02,\n",
       "         -3.8005e-02, -2.7232e-02, -2.7236e-02, -7.9745e-02,  7.4413e-03,\n",
       "          9.5555e-04,  2.6702e-03,  6.4233e-03,  2.9464e-02, -2.4807e-02,\n",
       "          4.9826e-02,  7.7195e-02,  2.9932e-02, -3.2439e-02,  3.7582e-02,\n",
       "          5.7510e-02, -3.2887e-02, -1.8172e-02, -2.4403e-02,  1.1959e-01,\n",
       "         -3.2395e-03, -1.1700e-02, -6.5287e-02,  7.9087e-02,  6.0487e-02],\n",
       "        [-3.0621e-01,  6.4465e-01, -2.8543e-01,  5.6573e-01, -4.2993e-01,\n",
       "         -1.0852e+00,  1.2997e+00,  1.5104e+00, -5.8759e-01, -4.2063e-01,\n",
       "         -2.8488e-01, -2.5938e-01, -9.5989e-01,  3.7014e-02, -1.0439e+00,\n",
       "         -2.6326e-01, -4.5191e-01,  3.0713e-01, -1.1873e+00, -4.8321e-01,\n",
       "          4.4779e-01,  1.0170e-01,  8.9827e-01,  5.2102e-02, -2.0555e-01,\n",
       "         -6.7875e-01, -1.3017e-01, -8.0803e-01, -1.7107e+00,  1.3166e-01,\n",
       "         -4.0269e-01, -4.8085e-01,  6.1520e-01,  1.6061e-01,  1.9370e-01,\n",
       "          2.7037e-01,  1.2078e+00,  2.8429e-01, -4.7460e-01,  8.0277e-02,\n",
       "          3.3416e-01, -1.9333e-01, -1.2267e+00, -1.3190e-01,  1.7839e+00,\n",
       "         -2.4173e-01,  1.2665e-01, -2.8854e-01,  8.7162e-01,  1.1330e+00],\n",
       "        [-1.4940e-01,  3.9882e-01, -5.8208e-01,  3.5881e-01, -1.5049e+00,\n",
       "         -6.2023e-01,  6.8851e-01,  1.7058e+00, -1.1012e+00, -9.0304e-01,\n",
       "         -4.0879e-01, -7.1755e-01, -1.0271e-01,  4.5623e-01, -1.2832e+00,\n",
       "          3.2191e-01, -1.8925e-01,  6.2124e-01, -9.1422e-01, -1.3725e-01,\n",
       "         -4.9971e-01, -3.3365e-01,  2.6398e-01,  3.0748e-01, -4.5637e-01,\n",
       "         -3.3765e-01, -9.4671e-01, -2.1308e-01, -1.8699e+00, -3.1114e-01,\n",
       "         -3.4580e-01, -2.2223e-01, -4.6640e-01,  2.6124e-01,  1.5991e-01,\n",
       "          1.5348e+00,  9.8096e-01, -1.0196e-01, -1.4515e+00,  5.4389e-01,\n",
       "          1.1192e+00, -5.3778e-01, -7.9007e-01,  8.4649e-02,  1.5518e+00,\n",
       "          1.4585e-01, -3.1152e-02, -6.0408e-01,  6.2817e-01,  1.4800e-01],\n",
       "        [ 1.4175e-02,  8.8611e-02, -1.0235e-01,  4.9774e-02, -8.2427e-02,\n",
       "         -7.4523e-02,  1.0710e-01,  1.9452e-01, -2.0091e-01,  1.7209e-02,\n",
       "         -7.6283e-02, -1.1773e-01, -1.8396e-03,  8.8503e-02, -1.2513e-01,\n",
       "          1.3143e-02,  1.0555e-01, -7.9698e-02, -2.1343e-01, -7.1883e-02,\n",
       "          1.8193e-02,  3.3127e-02,  1.7550e-01, -1.0086e-02,  5.7470e-02,\n",
       "         -3.9653e-02, -3.0909e-02,  1.7024e-02, -1.6469e-01, -1.6113e-03,\n",
       "         -2.0127e-02,  4.3523e-02,  8.1578e-03,  4.8298e-02, -6.6572e-02,\n",
       "          8.3306e-02,  2.1670e-01,  8.2974e-02, -3.5272e-02,  7.4022e-02,\n",
       "          1.1226e-01, -1.3454e-01, -6.4366e-02, -2.6904e-02,  1.9963e-01,\n",
       "         -7.6993e-03,  5.2687e-03, -9.1817e-02,  1.9242e-01,  8.7626e-02],\n",
       "        [-9.2548e-01,  4.8718e-01, -9.5494e-01,  6.2592e-01,  9.7916e-01,\n",
       "         -8.5361e-01,  3.7364e-01, -2.2617e-01, -3.7135e-01,  1.9480e+00,\n",
       "         -6.3236e-01, -9.3207e-02, -4.1347e-01, -3.1717e-01, -2.3561e-01,\n",
       "         -5.6648e-01, -1.8553e-01, -8.0785e-01, -5.8043e-01,  5.3179e-01,\n",
       "          8.6819e-01,  1.2484e+00, -4.2200e-01, -1.2401e+00,  5.3186e-01,\n",
       "         -9.4688e-01,  1.1060e+00, -2.5235e-01, -8.7749e-01, -8.8339e-01,\n",
       "         -1.3680e-01,  8.7286e-01,  4.3324e-01,  7.2614e-01, -8.7064e-01,\n",
       "          5.6095e-01,  1.9967e+00,  2.1736e-01,  5.4276e-01,  1.0075e+00,\n",
       "         -3.9027e-01, -2.8156e-01,  3.3633e-01,  4.6943e-02,  1.8344e+00,\n",
       "          2.7343e-01, -6.6844e-01, -6.2615e-01,  1.4373e+00,  6.5925e-01],\n",
       "        [-2.3083e-01,  4.5517e-01, -1.6959e+00, -4.1439e-01, -1.8894e+00,\n",
       "         -2.2464e+00,  6.3886e-01,  1.7091e+00, -1.1853e+00, -1.9959e+00,\n",
       "         -3.0330e-01, -8.2516e-01,  1.8953e-01,  8.4076e-01, -1.9309e+00,\n",
       "          1.0831e+00, -3.9191e-01,  1.0842e+00, -1.1673e+00,  1.0297e+00,\n",
       "          1.7249e-01, -2.8173e-02,  2.7709e-01,  1.3057e+00, -1.6081e+00,\n",
       "          3.7027e-01, -6.1714e-01, -1.2935e+00, -2.5607e+00, -7.4485e-01,\n",
       "         -3.6898e-01, -7.1172e-01, -1.1946e+00, -4.7089e-01,  6.4488e-01,\n",
       "          1.9643e+00,  1.2617e+00, -1.9763e-01, -1.0594e+00,  6.5067e-01,\n",
       "          9.4935e-01, -1.0146e+00, -9.4749e-01, -4.2083e-01,  2.2628e+00,\n",
       "          2.1127e-01,  3.4639e-01, -1.4833e-01,  1.1371e+00, -5.0290e-01],\n",
       "        [ 4.3769e-01,  2.7234e-01, -4.8416e-01,  4.7119e-01, -7.7670e-01,\n",
       "         -7.9342e-01,  1.3140e+00,  1.6074e+00, -7.5850e-01, -3.9606e-01,\n",
       "         -4.6622e-01, -5.0094e-01, -5.0978e-01,  8.9514e-01, -1.2389e+00,\n",
       "          2.8834e-01, -7.7115e-02,  1.7982e-01, -1.3664e+00,  1.2182e-01,\n",
       "          1.3913e-01, -2.7449e-01,  1.0065e+00, -1.9338e-01, -1.2322e-01,\n",
       "         -1.2414e+00, -1.5777e-01, -7.1828e-01, -9.9502e-01,  1.9636e-01,\n",
       "         -3.5613e-02,  7.6604e-02,  1.7637e-01, -4.5138e-02,  4.0623e-01,\n",
       "          7.6285e-01,  8.9578e-01,  3.8712e-01, -6.0030e-01, -1.4070e-01,\n",
       "          7.1176e-01, -6.5622e-01, -4.5955e-01,  1.0741e-01,  1.3721e+00,\n",
       "         -9.6662e-02, -4.2565e-01, -3.5048e-01,  1.1561e+00,  9.5437e-01],\n",
       "        [ 1.0545e-02,  8.0386e-02, -1.4439e-01,  4.7728e-02, -1.2703e-01,\n",
       "         -1.6265e-01,  1.6834e-01,  2.4366e-01, -2.0492e-01, -8.0169e-02,\n",
       "         -9.7931e-02, -1.0705e-01, -3.7976e-02,  1.1522e-01, -1.8695e-01,\n",
       "          5.2020e-02, -4.5442e-04, -1.3715e-02, -2.2737e-01,  1.6815e-02,\n",
       "          4.5820e-02, -1.5459e-02,  1.4341e-01,  1.4359e-02, -1.0422e-02,\n",
       "         -9.1846e-02, -6.1477e-02, -6.9785e-02, -2.1980e-01,  2.3257e-02,\n",
       "          1.5973e-02, -1.6893e-03,  4.1292e-04,  2.5778e-02, -1.6654e-02,\n",
       "          1.3887e-01,  1.8635e-01,  1.0221e-01, -1.2574e-01,  4.9700e-02,\n",
       "          1.2562e-01, -1.1550e-01, -1.0965e-01, -3.2560e-02,  2.3457e-01,\n",
       "         -1.6219e-02, -3.9882e-03, -8.4976e-02,  2.1054e-01,  1.0154e-01],\n",
       "        [-3.9917e-01,  8.4613e-01, -1.1099e+00,  1.2637e-01, -6.6402e-01,\n",
       "          6.8657e-01,  5.8058e-01,  1.0518e+00, -1.4389e+00,  4.6889e-01,\n",
       "         -1.4662e+00, -8.2565e-01,  6.1270e-03, -4.5170e-03, -7.3794e-01,\n",
       "          2.3977e-02,  1.0105e+00,  4.3898e-02, -1.3288e+00, -5.6597e-01,\n",
       "         -1.1504e-01, -1.3502e-01,  7.3685e-01,  2.0788e-01,  6.2126e-01,\n",
       "          7.2667e-02, -2.3174e-01, -2.1129e-02, -1.9934e+00,  6.0592e-02,\n",
       "          1.7630e-02,  3.6559e-01, -5.8294e-02,  7.2102e-01, -4.7979e-01,\n",
       "          3.9235e-01,  7.1295e-01,  5.2327e-01, -5.0557e-01,  6.0488e-01,\n",
       "          5.1020e-01, -4.6800e-01, -6.2693e-01, -1.6852e-01,  1.7989e-01,\n",
       "          5.2139e-01,  5.1848e-01, -5.1783e-01,  5.1373e-01,  5.1424e-01],\n",
       "        [ 1.5616e-02,  4.2546e-02, -3.6282e-02,  5.3125e-03, -1.7461e-02,\n",
       "         -4.3514e-02,  6.0714e-02,  1.0788e-01, -6.3329e-02, -4.7741e-02,\n",
       "          1.5846e-03, -4.3413e-02, -1.2932e-02,  2.8090e-02, -5.9462e-02,\n",
       "          1.8915e-02, -1.7638e-03,  3.6835e-03, -6.9455e-02,  1.4592e-02,\n",
       "          2.7034e-05,  2.5678e-03,  5.2820e-02,  2.5385e-02, -2.2963e-02,\n",
       "         -2.8496e-03, -4.7680e-02,  4.7151e-03, -3.6912e-02,  9.5188e-03,\n",
       "          1.3997e-02,  7.0269e-04, -1.6481e-03, -1.6128e-02, -4.0201e-03,\n",
       "          3.0634e-02,  5.1505e-02,  4.1849e-03, -5.4505e-02,  6.7386e-03,\n",
       "          2.4124e-02, -2.6199e-02, -3.9401e-02, -1.0752e-02,  5.4850e-02,\n",
       "         -1.6872e-02,  3.8183e-03, -1.9654e-02,  3.7791e-02,  3.9371e-02],\n",
       "        [-4.1279e-01,  7.8441e-01, -6.8442e-01,  2.3608e-01, -2.2525e-01,\n",
       "         -8.1625e-01,  7.6166e-01,  1.3317e+00, -6.5406e-01,  1.1435e-01,\n",
       "         -6.8773e-01, -4.3525e-01, -5.9425e-01,  2.0875e-01, -1.1555e+00,\n",
       "         -2.2769e-01,  3.4292e-01,  9.9292e-02, -1.3711e+00,  1.1947e-01,\n",
       "          3.3908e-01, -5.9826e-02,  6.4079e-01, -8.9567e-02, -1.2501e-03,\n",
       "         -7.7827e-01, -1.1553e-01, -6.8239e-01, -1.7213e+00, -2.4355e-01,\n",
       "         -2.8331e-01, -4.1628e-02,  1.2843e-01,  1.8415e-01,  6.0835e-02,\n",
       "          2.7201e-01,  1.1367e+00,  3.5175e-01, -1.1820e-01,  4.3125e-01,\n",
       "          2.5683e-01, -4.4960e-01, -5.5351e-01,  1.1238e-01,  1.2144e+00,\n",
       "         -7.9461e-02, -5.8358e-02, -3.3920e-01,  7.9093e-01,  7.1121e-01],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "input_dim = embeddings.EMBEDDING_SIZE\n",
    "hidden_dim = 300\n",
    "output_dim = len(word2vec_manager._model.wv.key_to_index)\n",
    "seq_length = SEQ_LENGTH \n",
    "\n",
    "generator = Generator(input_size=input_dim, hidden_size=hidden_dim, output_size=output_dim, seq_length=seq_length)\n",
    "discriminator = Discriminator(input_dim=input_dim, hidden_dim=hidden_dim, seq_length=seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator has 2485454 parameters\n",
      "Discriminator has 422701 parameters\n"
     ]
    }
   ],
   "source": [
    "discrim_params = list(discriminator.parameters())\n",
    "gen_params = list(generator.parameters())\n",
    "num_params_gen = sum([np.prod(p.size()) for p in gen_params])\n",
    "num_params_discrim = sum([np.prod(p.size()) for p in discrim_params])\n",
    "print(f'Generator has {num_params_gen} parameters')\n",
    "print(f'Discriminator has {num_params_discrim} parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 4, 6854])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "input.size(-1) must be equal to input_size. Expected 50, got 6854",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/johnhenryrudden/School/CS4120/Shaky_N_GANs/train_gan.ipynb Cell 10\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/johnhenryrudden/School/CS4120/Shaky_N_GANs/train_gan.ipynb#X13sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39m# need to detach generated data from the graph to avoid training the generator\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/johnhenryrudden/School/CS4120/Shaky_N_GANs/train_gan.ipynb#X13sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m \u001b[39mprint\u001b[39m(generated_data\u001b[39m.\u001b[39mshape)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/johnhenryrudden/School/CS4120/Shaky_N_GANs/train_gan.ipynb#X13sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m fake_loss \u001b[39m=\u001b[39m loss(discriminator(generated_data\u001b[39m.\u001b[39;49mdetach()), fake_labels)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/johnhenryrudden/School/CS4120/Shaky_N_GANs/train_gan.ipynb#X13sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m total_loss \u001b[39m=\u001b[39m real_loss \u001b[39m+\u001b[39m fake_loss\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/johnhenryrudden/School/CS4120/Shaky_N_GANs/train_gan.ipynb#X13sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m total_loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/anaconda3/envs/shaky_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/School/CS4120/Shaky_N_GANs/gan.py:40\u001b[0m, in \u001b[0;36mDiscriminator.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m     39\u001b[0m     \u001b[39m# Assuming x is of shape [batch_size, seq_length, input_dim]\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m     lstm_out, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlstm(x)\n\u001b[1;32m     41\u001b[0m     last_time_step_out \u001b[39m=\u001b[39m lstm_out[:, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, :]\n\u001b[1;32m     42\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc(last_time_step_out)\n",
      "File \u001b[0;32m~/anaconda3/envs/shaky_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/shaky_env/lib/python3.11/site-packages/torch/nn/modules/rnn.py:810\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    806\u001b[0m     \u001b[39m# Each batch of the hidden state should match the input sequence that\u001b[39;00m\n\u001b[1;32m    807\u001b[0m     \u001b[39m# the user believes he/she is passing in.\u001b[39;00m\n\u001b[1;32m    808\u001b[0m     hx \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[0;32m--> 810\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcheck_forward_args(\u001b[39minput\u001b[39;49m, hx, batch_sizes)\n\u001b[1;32m    811\u001b[0m \u001b[39mif\u001b[39;00m batch_sizes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    812\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39mlstm(\u001b[39minput\u001b[39m, hx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_weights, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers,\n\u001b[1;32m    813\u001b[0m                       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbidirectional, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_first)\n",
      "File \u001b[0;32m~/anaconda3/envs/shaky_env/lib/python3.11/site-packages/torch/nn/modules/rnn.py:730\u001b[0m, in \u001b[0;36mLSTM.check_forward_args\u001b[0;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[1;32m    725\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcheck_forward_args\u001b[39m(\u001b[39mself\u001b[39m,  \u001b[39m# type: ignore[override]\u001b[39;00m\n\u001b[1;32m    726\u001b[0m                        \u001b[39minput\u001b[39m: Tensor,\n\u001b[1;32m    727\u001b[0m                        hidden: Tuple[Tensor, Tensor],\n\u001b[1;32m    728\u001b[0m                        batch_sizes: Optional[Tensor],\n\u001b[1;32m    729\u001b[0m                        ):\n\u001b[0;32m--> 730\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcheck_input(\u001b[39minput\u001b[39;49m, batch_sizes)\n\u001b[1;32m    731\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_hidden_size(hidden[\u001b[39m0\u001b[39m], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_expected_hidden_size(\u001b[39minput\u001b[39m, batch_sizes),\n\u001b[1;32m    732\u001b[0m                            \u001b[39m'\u001b[39m\u001b[39mExpected hidden[0] size \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m    733\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_hidden_size(hidden[\u001b[39m1\u001b[39m], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_expected_cell_size(\u001b[39minput\u001b[39m, batch_sizes),\n\u001b[1;32m    734\u001b[0m                            \u001b[39m'\u001b[39m\u001b[39mExpected hidden[1] size \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/shaky_env/lib/python3.11/site-packages/torch/nn/modules/rnn.py:218\u001b[0m, in \u001b[0;36mRNNBase.check_input\u001b[0;34m(self, input, batch_sizes)\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    215\u001b[0m         \u001b[39m'\u001b[39m\u001b[39minput must have \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m dimensions, got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    216\u001b[0m             expected_input_dim, \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdim()))\n\u001b[1;32m    217\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_size \u001b[39m!=\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m--> 218\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    219\u001b[0m         \u001b[39m'\u001b[39m\u001b[39minput.size(-1) must be equal to input_size. Expected \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    220\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_size, \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: input.size(-1) must be equal to input_size. Expected 50, got 6854"
     ]
    }
   ],
   "source": [
    "# Train models\n",
    "import torch.optim as optim\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.001\n",
    "num_epochs = 2\n",
    "batch_size = 4\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "temperature = 0.5\n",
    "\n",
    "# Init optimizers\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=learning_rate)\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=learning_rate)\n",
    "\n",
    "# Init loss functions\n",
    "loss = torch.nn.BCELoss()\n",
    "\n",
    "# data loader\n",
    "dataloader = create_embedding_dataloader(tokenized_sentences, word2vec_manager, seq_length=SEQ_LENGTH, batch_size=batch_size)\n",
    "dataloader = iter(dataloader)\n",
    "\n",
    "for _ in range(num_epochs):\n",
    "    for real_data in dataloader:\n",
    "        batch_size = real_data.size(0)\n",
    "\n",
    "        # gen fake data\n",
    "        noise = torch.randn(batch_size, seq_length, input_dim)\n",
    "        generated_data = generator(noise, temperature)\n",
    "        fake_labels = torch.zeros(batch_size, 1)\n",
    "\n",
    "        # train discriminator\n",
    "        optimizer_D.zero_grad()\n",
    "        real_labels = torch.ones(batch_size, 1)\n",
    "        real_loss = loss(discriminator(real_data), real_labels)\n",
    "        # need to detach generated data from the graph to avoid training the generator\n",
    "        print(generated_data.shape)\n",
    "        fake_loss = loss(discriminator(generated_data.detach()), fake_labels)\n",
    "        total_loss = real_loss + fake_loss\n",
    "        total_loss.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        # train generator\n",
    "        optimizer_G.zero_grad()\n",
    "        fake_pred = discriminator(fake_data)\n",
    "        gen_loss = loss(fake_pred, real_labels)\n",
    "        gen_loss.backward()\n",
    "        optimizer_G.step()\n",
    "        \n",
    "    print(f'Generator loss: {gen_loss}')\n",
    "    print(f'Discriminator loss: {total_loss}')\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shaky_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

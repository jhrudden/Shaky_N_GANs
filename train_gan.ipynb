{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johnhenryrudden/anaconda3/envs/shaky_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/johnhenryrudden/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from embeddings import WordEmbeddingManager, create_embedding_dataloader\n",
    "import embeddings\n",
    "import utils\n",
    "from gan import Generator, Discriminator, train as train_gan\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = 'data/raw_train.txt'\n",
    "tokenized_sentences = utils.process_training_data(TRAIN_PATH)\n",
    "# Only use the first 10,000 sentences for now\n",
    "tokenized_sentences = tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longest sentence has 20 tokens\n"
     ]
    }
   ],
   "source": [
    "longest_sentence = max(tokenized_sentences, key=len)\n",
    "print(f'Longest sentence has {len(longest_sentence)} tokens')\n",
    "SEQ_LENGTH = len(longest_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully from data/word2vec.model\n"
     ]
    }
   ],
   "source": [
    "WORD2VEC_MODEL_PATH = 'data/word2vec.model'\n",
    "word2vec_manager = WordEmbeddingManager(WORD2VEC_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Double check one hot encoding and decoding is working as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader = create_embedding_dataloader(tokenized_sentences, word2vec_manager, seq_length=SEQ_LENGTH, batch_size=4, encoding_method=\"one_hot\", verbose=True)\n",
    "# batch = next(iter(dataloader))\n",
    "# print(f'first batch sentence: {batch[0]}')\n",
    "# encoded_first_sentence = batch[0]\n",
    "# decoded_first_sentence = [word2vec_manager.decode_one_hot(encoded_token) for encoded_token in encoded_first_sentence]\n",
    "# decoded_first_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence_to_encode = ['That', 'you', 'have', \"ta'en\", 'a', 'tardy', '<UNK>', 'here', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
    "# encoded_sentence = [word2vec_manager.one_hot_encode(word) for word in sentence_to_encode]\n",
    "# decoded = [word2vec_manager.decode_one_hot(one_hot) for one_hot in encoded_sentence]\n",
    "# encoded_sentence,decoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "seq_length = SEQ_LENGTH \n",
    "\n",
    "# Generator first\n",
    "\n",
    "gen_input_dim = embeddings.EMBEDDING_SIZE\n",
    "gen_hidden_dim = 300\n",
    "\n",
    "# add 1 to output dim to account for padding token\n",
    "gen_output_dim = len(word2vec_manager._model.wv.key_to_index) + 1\n",
    "\n",
    "generator = Generator(input_size=gen_input_dim, hidden_size=gen_hidden_dim, output_size=gen_output_dim, seq_length=seq_length)\n",
    "\n",
    "\n",
    "# Discriminator\n",
    "# Discriminator input is the same as the generator output (the generated next token probability distribution)\n",
    "discrim_input_dim = gen_output_dim\n",
    "discrim_hidden_dim = 100\n",
    "\n",
    "discriminator = Discriminator(input_dim=discrim_input_dim, hidden_dim=discrim_hidden_dim, seq_length=seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator has 2485755 parameters\n",
      "Discriminator has 2782901 parameters\n"
     ]
    }
   ],
   "source": [
    "discrim_params = list(discriminator.parameters())\n",
    "gen_params = list(generator.parameters())\n",
    "num_params_gen = sum([np.prod(p.size()) for p in gen_params])\n",
    "num_params_discrim = sum([np.prod(p.size()) for p in discrim_params])\n",
    "print(f'Generator has {num_params_gen} parameters')\n",
    "print(f'Discriminator has {num_params_discrim} parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train models\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.0001\n",
    "batch_size = 4\n",
    "temperature = 1.0\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "num_epochs = 1\n",
    "TRAIN_ON_FRAC = 0.1\n",
    "train_size = int(TRAIN_ON_FRAC * len(tokenized_sentences))\n",
    "train_sents, _ = train_test_split(tokenized_sentences, train_size=TRAIN_ON_FRAC, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentences(generator, temp):\n",
    "    gens = []\n",
    "    for i in range(10):\n",
    "        noise = torch.randn(1, seq_length, gen_input_dim)\n",
    "        generated_data = generator(noise, temperature, hard=False)\n",
    "        argmaxs = torch.argmax(generated_data[0], dim=1)\n",
    "        generated_sentence = [word2vec_manager.index_to_word(index) for index in argmaxs]\n",
    "        gens.append(\" \".join(generated_sentence).replace(\"<PAD>\", \"\"))\n",
    "    return gens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr: 0.001, batch_size: 4, temp: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   0%|          | 0/731 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1 | Generator Loss: 8.4974 | Discriminator Loss: 0.0008: 100%|██████████| 731/731 [01:40<00:00,  7.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr: 0.001, batch_size: 4, temp: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1 | Generator Loss: 8.1949 | Discriminator Loss: 0.0008: 100%|██████████| 731/731 [01:43<00:00,  7.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr: 0.001, batch_size: 4, temp: 1.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1 | Generator Loss: 5.7720 | Discriminator Loss: 0.0030: 100%|██████████| 731/731 [01:44<00:00,  7.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr: 0.001, batch_size: 8, temp: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1 | Generator Loss: 4.3503 | Discriminator Loss: 0.7030: 100%|██████████| 366/366 [02:09<00:00,  2.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr: 0.001, batch_size: 8, temp: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1 | Generator Loss: 1.9736 | Discriminator Loss: 0.2262: 100%|██████████| 366/366 [01:40<00:00,  3.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr: 0.001, batch_size: 8, temp: 1.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1 | Generator Loss: 6.0809 | Discriminator Loss: 0.0031: 100%|██████████| 366/366 [01:42<00:00,  3.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr: 0.001, batch_size: 16, temp: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1 | Generator Loss: 0.7343 | Discriminator Loss: 1.3689: 100%|██████████| 183/183 [07:20<00:00,  2.41s/it]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr: 0.001, batch_size: 16, temp: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1 | Generator Loss: 1.5237 | Discriminator Loss: 0.6204: 100%|██████████| 183/183 [01:08<00:00,  2.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr: 0.001, batch_size: 16, temp: 1.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1 | Generator Loss: 1.1824 | Discriminator Loss: 0.5345: 100%|██████████| 183/183 [01:08<00:00,  2.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr: 0.0005, batch_size: 4, temp: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1 | Generator Loss: 4.2601 | Discriminator Loss: 0.0939: 100%|██████████| 731/731 [01:45<00:00,  6.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr: 0.0005, batch_size: 4, temp: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1 | Generator Loss: 5.3449 | Discriminator Loss: 0.3717: 100%|██████████| 731/731 [01:47<00:00,  6.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr: 0.0005, batch_size: 4, temp: 1.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1 | Generator Loss: 1.2972 | Discriminator Loss: 1.6144: 100%|██████████| 731/731 [01:44<00:00,  6.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr: 0.0005, batch_size: 8, temp: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1 | Generator Loss: 2.9739 | Discriminator Loss: 0.0768: 100%|██████████| 366/366 [01:36<00:00,  3.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr: 0.0005, batch_size: 8, temp: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1 | Generator Loss: 0.6228 | Discriminator Loss: 1.3979: 100%|██████████| 366/366 [01:37<00:00,  3.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr: 0.0005, batch_size: 8, temp: 1.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1 | Generator Loss: 2.1383 | Discriminator Loss: 0.2904: 100%|██████████| 366/366 [01:44<00:00,  3.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr: 0.0005, batch_size: 16, temp: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1 | Generator Loss: 0.7921 | Discriminator Loss: 1.3552: 100%|██████████| 183/183 [01:12<00:00,  2.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr: 0.0005, batch_size: 16, temp: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1 | Generator Loss: 0.7260 | Discriminator Loss: 1.3720: 100%|██████████| 183/183 [01:08<00:00,  2.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr: 0.0005, batch_size: 16, temp: 1.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1 | Generator Loss: 0.7579 | Discriminator Loss: 1.3522: 100%|██████████| 183/183 [01:07<00:00,  2.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr: 0.0001, batch_size: 4, temp: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1 | Generator Loss: 0.7977 | Discriminator Loss: 1.4066: 100%|██████████| 731/731 [01:44<00:00,  7.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr: 0.0001, batch_size: 4, temp: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1 | Generator Loss: 1.4879 | Discriminator Loss: 1.0192: 100%|██████████| 731/731 [01:52<00:00,  6.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr: 0.0001, batch_size: 4, temp: 1.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1 | Generator Loss: 0.7224 | Discriminator Loss: 1.3949: 100%|██████████| 731/731 [01:57<00:00,  6.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr: 0.0001, batch_size: 8, temp: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1 | Generator Loss: 0.7290 | Discriminator Loss: 0.9488: 100%|██████████| 366/366 [01:37<00:00,  3.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr: 0.0001, batch_size: 8, temp: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1 | Generator Loss: 0.7040 | Discriminator Loss: 1.3744:   6%|▋         | 23/366 [00:06<01:37,  3.51it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/johnhenryrudden/School/CS4120/Shaky_N_GANs/train_gan.ipynb Cell 13\u001b[0m line \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/johnhenryrudden/School/CS4120/Shaky_N_GANs/train_gan.ipynb#X15sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m discriminator \u001b[39m=\u001b[39m Discriminator(input_dim\u001b[39m=\u001b[39mdiscrim_input_dim, hidden_dim\u001b[39m=\u001b[39mdiscrim_hidden_dim, seq_length\u001b[39m=\u001b[39mseq_length)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/johnhenryrudden/School/CS4120/Shaky_N_GANs/train_gan.ipynb#X15sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mlr: \u001b[39m\u001b[39m{\u001b[39;00mlr\u001b[39m}\u001b[39;00m\u001b[39m, batch_size: \u001b[39m\u001b[39m{\u001b[39;00mbatch_size\u001b[39m}\u001b[39;00m\u001b[39m, temp: \u001b[39m\u001b[39m{\u001b[39;00mtemp\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/johnhenryrudden/School/CS4120/Shaky_N_GANs/train_gan.ipynb#X15sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m g_loss, d_loss \u001b[39m=\u001b[39mtrain_gan(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/johnhenryrudden/School/CS4120/Shaky_N_GANs/train_gan.ipynb#X15sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     generator\u001b[39m=\u001b[39;49mgenerator,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/johnhenryrudden/School/CS4120/Shaky_N_GANs/train_gan.ipynb#X15sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     discriminator\u001b[39m=\u001b[39;49mdiscriminator,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/johnhenryrudden/School/CS4120/Shaky_N_GANs/train_gan.ipynb#X15sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     tokenized_sentences\u001b[39m=\u001b[39;49mtrain_sents,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/johnhenryrudden/School/CS4120/Shaky_N_GANs/train_gan.ipynb#X15sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     word2vec_manager\u001b[39m=\u001b[39;49mword2vec_manager,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/johnhenryrudden/School/CS4120/Shaky_N_GANs/train_gan.ipynb#X15sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     seq_length\u001b[39m=\u001b[39;49mSEQ_LENGTH,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/johnhenryrudden/School/CS4120/Shaky_N_GANs/train_gan.ipynb#X15sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     generator_input_features\u001b[39m=\u001b[39;49mgen_input_dim,  \u001b[39m# Updated parameter name\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/johnhenryrudden/School/CS4120/Shaky_N_GANs/train_gan.ipynb#X15sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     num_epochs\u001b[39m=\u001b[39;49mnum_epochs,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/johnhenryrudden/School/CS4120/Shaky_N_GANs/train_gan.ipynb#X15sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/johnhenryrudden/School/CS4120/Shaky_N_GANs/train_gan.ipynb#X15sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     noise_sample_method\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mnormal\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/johnhenryrudden/School/CS4120/Shaky_N_GANs/train_gan.ipynb#X15sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     gumbel_hard\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/johnhenryrudden/School/CS4120/Shaky_N_GANs/train_gan.ipynb#X15sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     learning_rate\u001b[39m=\u001b[39;49mlr,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/johnhenryrudden/School/CS4120/Shaky_N_GANs/train_gan.ipynb#X15sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     temperature\u001b[39m=\u001b[39;49mtemp,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/johnhenryrudden/School/CS4120/Shaky_N_GANs/train_gan.ipynb#X15sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     encoding_method\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mone_hot\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/johnhenryrudden/School/CS4120/Shaky_N_GANs/train_gan.ipynb#X15sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     device\u001b[39m=\u001b[39;49mdevice,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/johnhenryrudden/School/CS4120/Shaky_N_GANs/train_gan.ipynb#X15sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     debug\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/johnhenryrudden/School/CS4120/Shaky_N_GANs/train_gan.ipynb#X15sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/johnhenryrudden/School/CS4120/Shaky_N_GANs/train_gan.ipynb#X15sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m gens \u001b[39m=\u001b[39m generate_sentences(generator, temp)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/johnhenryrudden/School/CS4120/Shaky_N_GANs/train_gan.ipynb#X15sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m results\u001b[39m.\u001b[39mappend((lr, batch_size, temp, gens, g_loss, d_loss))\n",
      "File \u001b[0;32m~/School/CS4120/Shaky_N_GANs/gan.py:195\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(generator, discriminator, tokenized_sentences, word2vec_manager, seq_length, generator_input_features, num_epochs, batch_size, learning_rate, temperature, temp_decay_rate, gumbel_hard, encoding_method, noise_sample_method, device, debug)\u001b[0m\n\u001b[1;32m    193\u001b[0m optimizer_G\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m    194\u001b[0m noise \u001b[39m=\u001b[39m generate_noise((batch_size, seq_length, generator_input_features), noise_sample_method)\n\u001b[0;32m--> 195\u001b[0m generated_data \u001b[39m=\u001b[39m generator(noise, temperature, hard\u001b[39m=\u001b[39;49mgumbel_hard)  \n\u001b[1;32m    196\u001b[0m fake_pred \u001b[39m=\u001b[39m discriminator(generated_data)\n\u001b[1;32m    197\u001b[0m fake_labels \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mones(batch_size, \u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/shaky_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/School/CS4120/Shaky_N_GANs/gan.py:75\u001b[0m, in \u001b[0;36mGenerator.forward\u001b[0;34m(self, z, temp, hard)\u001b[0m\n\u001b[1;32m     73\u001b[0m linear \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc(lstm_out)\n\u001b[1;32m     74\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mls(linear)\n\u001b[0;32m---> 75\u001b[0m gumbel_softmaxed_logits \u001b[39m=\u001b[39m [gumbel_softmax(logits[:,i,:], temp, hard) \u001b[39mfor\u001b[39;49;00m i \u001b[39min\u001b[39;49;00m \u001b[39mrange\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mseq_size)]\n\u001b[1;32m     76\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mstack(gumbel_softmaxed_logits, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/School/CS4120/Shaky_N_GANs/gan.py:75\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     73\u001b[0m linear \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc(lstm_out)\n\u001b[1;32m     74\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mls(linear)\n\u001b[0;32m---> 75\u001b[0m gumbel_softmaxed_logits \u001b[39m=\u001b[39m [gumbel_softmax(logits[:,i,:], temp, hard) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mseq_size)]\n\u001b[1;32m     76\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mstack(gumbel_softmaxed_logits, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/School/CS4120/Shaky_N_GANs/gan.py:22\u001b[0m, in \u001b[0;36mgumbel_softmax\u001b[0;34m(logits, temperature, hard)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[39mImplements the gumbel softmax function described in https://arxiv.org/pdf/1611.04051.pdf\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[39m    hard (bool, optional): If True, the output will be one-hot encoded. Default is False.\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     21\u001b[0m U \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrand(logits\u001b[39m.\u001b[39mshape)\n\u001b[0;32m---> 22\u001b[0m G \u001b[39m=\u001b[39m \u001b[39m-\u001b[39mtorch\u001b[39m.\u001b[39;49mlog(\u001b[39m-\u001b[39;49mtorch\u001b[39m.\u001b[39;49mlog(U \u001b[39m+\u001b[39;49m \u001b[39m1e-20\u001b[39;49m) \u001b[39m+\u001b[39;49m \u001b[39m1e-20\u001b[39;49m)\n\u001b[1;32m     25\u001b[0m y \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39msoftmax((logits \u001b[39m+\u001b[39m G) \u001b[39m/\u001b[39m temperature, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     26\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m hard:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for lr in [0.001, 0.0005, 0.0001]:\n",
    "    for batch_size in [4, 8, 16]:\n",
    "        for temp in [0.5, 1.0, 1.5]:\n",
    "            # TODO: need to test trained models on validation set\n",
    "            # TODO: so this model needs percision recall and f1 score\n",
    "            generator = Generator(input_size=gen_input_dim, hidden_size=gen_hidden_dim, output_size=gen_output_dim, seq_length=seq_length)\n",
    "            discriminator = Discriminator(input_dim=discrim_input_dim, hidden_dim=discrim_hidden_dim, seq_length=seq_length)\n",
    "            print(f'lr: {lr}, batch_size: {batch_size}, temp: {temp}')\n",
    "            g_loss, d_loss =train_gan(\n",
    "                generator=generator,\n",
    "                discriminator=discriminator,\n",
    "                tokenized_sentences=train_sents,\n",
    "                word2vec_manager=word2vec_manager,\n",
    "                seq_length=SEQ_LENGTH,\n",
    "                generator_input_features=gen_input_dim,  # Updated parameter name\n",
    "                num_epochs=num_epochs,\n",
    "                batch_size=batch_size,\n",
    "                noise_sample_method=\"normal\",\n",
    "                gumbel_hard=True,\n",
    "                learning_rate=lr,\n",
    "                temperature=temp,\n",
    "                encoding_method=\"one_hot\",\n",
    "                device=device,\n",
    "                debug=False,\n",
    "            )\n",
    "            gens = generate_sentences(generator, temp)\n",
    "            results.append((lr, batch_size, temp, gens, g_loss, d_loss))\n",
    "\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.001,\n",
       "  4,\n",
       "  0.5,\n",
       "  ['Most heard heard heard heard heard heard heard heard heard heard heard heard heard heard heard heard heard heard heard',\n",
       "   'nor down down down down down down down down down down down down down down down down down down down',\n",
       "   'Paulina rashly heard heard heard heard heard heard heard heard heard heard heard heard heard heard heard heard heard heard',\n",
       "   'affecting bring down down down down down down down down down down down down down down down down down down',\n",
       "   'so down down down down down down down down down down down down down down down down down down down',\n",
       "   'pretty spoil heard heard heard heard heard heard heard heard heard heard heard heard heard heard heard heard heard heard',\n",
       "   'followed fawn down down down down down down down down down down down down down down down down down down',\n",
       "   'and welcome down down down down down down down down down down down down down down down down down down',\n",
       "   'scorn gasping down down down down down down down down down down down down down down down down down down',\n",
       "   'perjury down down down down down down down down down down down down down down down down down down down']),\n",
       " (0.001,\n",
       "  4,\n",
       "  1.0,\n",
       "  ['Keep stooping stooping stooping stooping stooping stooping stooping stooping stooping stooping stooping stooping stooping stooping stooping stooping stooping stooping stooping',\n",
       "   'merely stooping stooping stooping stooping stooping stooping stooping body stooping stooping stooping stooping stooping stooping stooping stooping stooping stooping stooping',\n",
       "   'violence : : : : : : : : : : : : : : : : : : :',\n",
       "   'apart stooping stooping stooping stooping stooping stooping stooping stooping stooping stooping stooping stooping stooping stooping stooping stooping stooping stooping stooping',\n",
       "   'corruption bread stooping body stooping stooping stooping stooping stooping stooping stooping stooping stooping stooping stooping stooping stooping stooping stooping stooping',\n",
       "   'heaviness ostentation : : : : : : : : : : : : : : : : : :',\n",
       "   'contented earnestness : : : : : : : : : : : : : : : : : :',\n",
       "   'unlike deadly wise : : : : : : : : : : : : : : : : :',\n",
       "   'magistrates stooping stooping stooping stooping stooping stooping stooping stooping stooping stooping stooping stooping stooping stooping stooping stooping stooping stooping stooping',\n",
       "   'One penitent strength stooping stooping stooping stooping stooping stooping stooping stooping stooping stooping stooping stooping stooping stooping stooping stooping stooping']),\n",
       " (0.001,\n",
       "  4,\n",
       "  1.5,\n",
       "  ['Now                   ',\n",
       "   'market-place any All any any any any any any any any any any any any any any any any any',\n",
       "   'any any any any any any any any any any any any any any any any any any any any',\n",
       "   'bad public                  ',\n",
       "   'camp pleased any any any any any any any any any any any any any any any any any any',\n",
       "   'licence lowly                  ',\n",
       "   'of playfellow                  ',\n",
       "   'conduit any any any any any any any any any any any any any any any any any any any',\n",
       "   'any any any any any any any any any any any any any any any any any any any any',\n",
       "   'dream whet any any any any any any any any any any any any any any any any any any']),\n",
       " (0.001,\n",
       "  8,\n",
       "  0.5,\n",
       "  ['Having                   ',\n",
       "   'Within letters butterfly awhile gossip cured patron Silence rubs committed ape tainted curst weaker wolf Mistress dateless saucy toucheth maid',\n",
       "   'win                   ',\n",
       "   'Find &                  ',\n",
       "   'letting                   ',\n",
       "   'does greater jest wrath drum lengthens chaste deadly apply yon self-same persons token valiant strengthen Scroop PAULINA power RUTLAND estate',\n",
       "   'aid                   ',\n",
       "   'copy                   ',\n",
       "   'praised                   ',\n",
       "   'promise                   ']),\n",
       " (0.001,\n",
       "  8,\n",
       "  1.0,\n",
       "  ['how BUCKINGHAM                  ',\n",
       "   'canon buildeth schoolboys closet private ugly methinks using well-seeming garments embrace Taking feeding well-spoken unking am awful minutes spy thanks',\n",
       "   'peaceful                   ',\n",
       "   \"weep'st succor strain holding honourable hearth shines             \",\n",
       "   'disinherited Unsheathe services nursed liver making tenderness barne rail midnight countenances absence protector brook doings Given discern using vows hasty',\n",
       "   'mightst ice beats beget curse sear form breaks mistook breeder marshal shoot Worcester Told gentle its contract numbering spare liquid',\n",
       "   \"At choice meet Verona birth apply falcon her throws kind sea shrink sings o'erbear in still orator Erroneous to't spite\",\n",
       "   'performed popular                  ',\n",
       "   \"greet frost substantial alter visiting houses wing harvest Jove through Ladies tie rear spectators consent behind alter beguiled target take't\",\n",
       "   'wink morrow , Arise                ']),\n",
       " (0.001,\n",
       "  8,\n",
       "  1.5,\n",
       "  ['chaos serving-creature shamest sincerity believed peace free peace free free free free free free free peace free free free free',\n",
       "   'serpent feeble entreats peace free peace free free free free free peace free free free peace free free free free',\n",
       "   'peace peace free free free free free free free free peace free free peace free peace peace free free free',\n",
       "   'Bad wrapt free free free free free free free free free peace free free free peace free free free free',\n",
       "   'cruel peace free peace free free free free free free peace free peace free free free free free free free',\n",
       "   'bitter gillyvors tiger free free free free peace free peace free free free free free free free peace free free',\n",
       "   'magistrate show strengths free free free free free free free free free free free free free free free free free',\n",
       "   'nobleness Hereford dishonours dealings free free free free free free free peace free free free free free free free free',\n",
       "   'pearl point peace peace free free free free free free free free free free free free free free free free',\n",
       "   'famish storm prisoners flattery free free free free free free free free peace free free free free free free free']),\n",
       " (0.001,\n",
       "  16,\n",
       "  0.5,\n",
       "  ['summon crosses                  ',\n",
       "   'dragon                   ',\n",
       "   'Kind burns                  ',\n",
       "   'spring Compare                  ',\n",
       "   'climate son                  ',\n",
       "   'priests                   ',\n",
       "   'Receive                   ',\n",
       "   'winter wither                  ',\n",
       "   'Gremio thy                  ',\n",
       "   'outward brawling                  ']),\n",
       " (0.001,\n",
       "  16,\n",
       "  1.0,\n",
       "  ['manacles                   ',\n",
       "   'barne ;                  ',\n",
       "   'disdain them                  ',\n",
       "   'benefit Tewksbury                  ',\n",
       "   'alter                   ',\n",
       "   'at                   ',\n",
       "   'hen                   ',\n",
       "   \"'True though                  \",\n",
       "   'mercy                   ',\n",
       "   'betake                   ']),\n",
       " (0.001,\n",
       "  16,\n",
       "  1.5,\n",
       "  ['Was cordial heed drawn                ',\n",
       "   'thereto inclination must                 ',\n",
       "   'fed dishes parlous repeat complaints               ',\n",
       "   'Lament strongly mildly somewhat                ',\n",
       "   'rigour rock nothings melancholy amiss famous never crutches            ',\n",
       "   'rod Hugh abused                 ',\n",
       "   'redress stoop fashions                 ',\n",
       "   'thirty chose freedom growth Phoebus transport              ',\n",
       "   'Tranio allegiance Must climb provided ribs              ',\n",
       "   'jar Mercy protect dissemble                ']),\n",
       " (0.0005,\n",
       "  4,\n",
       "  0.5,\n",
       "  ['ounce broke broke broke broke broke broke broke broke broke broke broke broke broke broke broke broke broke broke broke',\n",
       "   'lawful broke broke broke broke broke broke broke broke broke broke broke broke broke broke broke broke broke broke broke',\n",
       "   'schoolboys broke broke broke broke broke broke broke broke broke broke broke broke broke broke broke broke broke broke broke',\n",
       "   'clouded broke broke broke broke broke broke broke broke broke broke broke broke broke broke broke broke broke broke broke',\n",
       "   'contend tied broke broke broke broke broke broke broke broke broke broke broke broke broke broke broke broke broke broke',\n",
       "   'toss Hold broke broke broke broke broke broke broke broke broke broke broke broke broke broke broke broke broke ,',\n",
       "   '. on broke broke broke broke broke broke broke broke broke broke broke broke broke broke broke broke broke broke',\n",
       "   'fire function broke broke broke broke broke broke broke broke broke broke broke broke broke broke broke broke broke broke',\n",
       "   'terms broke broke broke broke broke broke broke broke broke broke broke broke broke broke broke broke broke broke broke',\n",
       "   'conduct broke broke broke broke broke broke broke broke broke broke broke broke broke broke broke broke broke broke broke']),\n",
       " (0.0005,\n",
       "  4,\n",
       "  1.0,\n",
       "  ['ear Therein lowly manner fingers baggage              ',\n",
       "   \"Still bringing chaos neighbours 'Courage breaths              \",\n",
       "   'lasts loyalty coast nuns remains Flavius rising harsh he           ',\n",
       "   'advice tongue vile bolder bled town capital Lie          robbery  ',\n",
       "   'excellence begging Shepherd issuing armed Fit news requires Englishman Quit          ',\n",
       "   \"clear Smile At Mariana gavest 'stand quench             Mopsa\",\n",
       "   \"ensign shelter receiving deeply uncrown befits stand'st distress Lives fancy champions help arise       \",\n",
       "   'baggage author foreign resolution                ',\n",
       "   'boundless pride hag land worthily waves Barbary blench contempt Death attach depends        ',\n",
       "   \"unthrifty wronged schoolmaster 'twere Montagues Sicils     he         \"]),\n",
       " (0.0005,\n",
       "  4,\n",
       "  1.5,\n",
       "  ['sails : : <UNK> : : : <UNK> : : <UNK> <UNK> : : : <UNK> <UNK> : <UNK> :',\n",
       "   'the Barbary sea-side wean When unrest Caius words monster sick expressly Dear possessions threaten Being sought article debase funeral promise',\n",
       "   'likely : <UNK> <UNK> <UNK> <UNK> : : : : <UNK> : : : : <UNK> : : <UNK> <UNK>',\n",
       "   'MAMILLIUS : : <UNK> : : : : <UNK> : : : : : : : <UNK> : : :',\n",
       "   \"what touch beast pricking advertised letters courteous fought redress solemnity isle stoutness mirror contradiction deaf go'st beshrew loving narrow Wife\",\n",
       "   \"Murderer blaze tailors succeed tumble murderer rank pinched 'she by-gone rod full ask sup To scratch hearth ANTIGONUS estimation Suffolk\",\n",
       "   'Rather shore lechery beggars dishonest changing placed claim defiance lengthen allies HERMIONE Capulet Commend served curds ballad-makers attempt shot rewards',\n",
       "   'natural liars : <UNK> <UNK> <UNK> : : : : : : : : <UNK> : : : : :',\n",
       "   'swells : : <UNK> : : : : : <UNK> : <UNK> : <UNK> : : : : : <UNK>',\n",
       "   'borough remedy : <UNK> : : : : : : : : : : : : <UNK> : : :']),\n",
       " (0.0005,\n",
       "  8,\n",
       "  0.5,\n",
       "  ['transported notwithstanding                  ',\n",
       "   'gyves Provided                  ',\n",
       "   'charms factious                  ',\n",
       "   'beguile rack                  ',\n",
       "   'Killing three                  ',\n",
       "   'bare-foot crimes                  ',\n",
       "   'cockatrice officers                  ',\n",
       "   \"'Bless Women                  \",\n",
       "   'Enjoy Volsces                  ',\n",
       "   'cap shapes     ,             ']),\n",
       " (0.0005,\n",
       "  8,\n",
       "  1.0,\n",
       "  ['Catesby off                  ',\n",
       "   'advantage                   ',\n",
       "   'beggarly commanded knightly                 ',\n",
       "   \"befall'n captives                  \",\n",
       "   \"fled 'Heart                  \",\n",
       "   'consul show                  ',\n",
       "   'ply commends                  ',\n",
       "   'fought seconds                  ',\n",
       "   'consume fourteen                ,  ',\n",
       "   'offices view bran                 ']),\n",
       " (0.0005,\n",
       "  8,\n",
       "  1.5,\n",
       "  ['fearful lovelier quit hundred monster merit pardons free kings sheer Fell blessings you beheld AUMERLE throughly attend difference side lass',\n",
       "   \"preparation horns look'st Prithee aid Frederick Thy Who unruly egg blemish kingly blunt already intents verdict choler parlous deal runn'st\",\n",
       "   \"motive At inclined guides Thursday rock Sit lineal Suggest pretty high'st malcontent sunder Wrong buried O met shrieks smile pine\",\n",
       "   \"Ere couldst Father perfection Percy declines 'The enjoin moody refer sue seems mutinies Came dreaded 'Twere highness our died doings\",\n",
       "   \"Reward hand bury hostile ladder contract mangled will't flag surely VINCENTIO pictures coronation process stern excels formerly inconstant vestal fairest\",\n",
       "   'April Europe carrion marks sends brotherhood carrion scourge scourge carrion scourge carrion scourge scourge carrion scourge carrion carrion carrion scourge',\n",
       "   'guiltless Again victory rash casque dry voices everlastingly watch be Not Sent Without me swear clutch Sicilia banquet goods himself',\n",
       "   'threaten eggs ends dishonest longs cousins twice breasts foundation warrant befal Flower poison fill kings Deserve school Lascivious brittle vantage',\n",
       "   'expostulate verge men redress Please time lack quarter commune easy can obscured forest ballads assist unkindness charters Bloody bird session',\n",
       "   'Naples plagues ocean happily fulfilled Westminster haunt needy cave shoot eagles HASTINGS miss brotherhood carrion scourge scourge scourge carrion scourge']),\n",
       " (0.0005,\n",
       "  16,\n",
       "  0.5,\n",
       "  ['swift accomplish                  ',\n",
       "   'Archbishop bawdy                  ',\n",
       "   'soonest my                  ',\n",
       "   'entreated deaths                  ',\n",
       "   \"nought O'er                  \",\n",
       "   'coats keep                  ',\n",
       "   'fitter Procure                  ',\n",
       "   'Happily patience                  ',\n",
       "   'snow greyhounds                  ',\n",
       "   'levied Told                  ']),\n",
       " (0.0005,\n",
       "  16,\n",
       "  1.0,\n",
       "  ['fancy Frederick                  ',\n",
       "   'wreaths consequence                  ',\n",
       "   'law receipt                  ',\n",
       "   'arrested bait                  ',\n",
       "   \"husbandry 'em                  \",\n",
       "   'goose infect                  ',\n",
       "   'solely smother                  ',\n",
       "   'orange think                  ',\n",
       "   'lass                   ',\n",
       "   'Under contradiction                  ']),\n",
       " (0.0005,\n",
       "  16,\n",
       "  1.5,\n",
       "  ['Thursday all                  ',\n",
       "   'Against admire                  ',\n",
       "   'bore                   ',\n",
       "   'alteration ministers                  ',\n",
       "   'be that                  ',\n",
       "   'birthright wrinkles                  ',\n",
       "   'stale                   ',\n",
       "   'crowns                   ',\n",
       "   'wandering                   ',\n",
       "   'seal                   ']),\n",
       " (0.0001,\n",
       "  4,\n",
       "  0.5,\n",
       "  ['contrive Florizel <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>',\n",
       "   'mum opposed <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>',\n",
       "   'crow posterity <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> the',\n",
       "   'Repent Speak <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>',\n",
       "   'hours Oh <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>',\n",
       "   'Unto Bring <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>',\n",
       "   'summers glass <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>',\n",
       "   'virtues beyond <UNK> <UNK> <UNK> <UNK> the <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>',\n",
       "   'gentle wearing <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>',\n",
       "   'apprehends FITZWATER <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>']),\n",
       " (0.0001,\n",
       "  4,\n",
       "  1.0,\n",
       "  ['called odds left                 ',\n",
       "   'sprung depth morsel                 ',\n",
       "   'creep fitted Ladies       beat          ',\n",
       "   'sky true conspire    benefit             ',\n",
       "   'grew weeping muddy                 ',\n",
       "   \"'Tell hit studied ?                \",\n",
       "   'wept thirst chapel                 and',\n",
       "   'silence thought portion         civil        ',\n",
       "   'the Teach Reason     weeping            ',\n",
       "   'greet Lovel ho    doubtful             ']),\n",
       " (0.0001,\n",
       "  4,\n",
       "  1.5,\n",
       "  ['slip privilege throats Help Hugh Hence mark fee Tongue-tied ostentation sooner sending Tarquin       ',\n",
       "   \"path should Give deceased Herbert shun 'shall witch medicine accept upon Destroy        \",\n",
       "   'rejoicing graves Tarpeian downright eyes selfsame deliberate Between losing gently proudest         ',\n",
       "   'meant wisdom bush trees Aumerle hoarse willing cloak majesty An   ,       ',\n",
       "   'events six entreaty Spoke ease pricking effect stands submit affects Crosby favour Thereof       ',\n",
       "   'Abbot hundred hearts grub madmen Sound Withal blessings  road subjects         ',\n",
       "   'minister follies devout hairs London helm Bootless wanton pricks torches worth         ',\n",
       "   'shalt Quick none murder Europe knowing passing calf hate haught          ',\n",
       "   \"things hurl Having Longer instruments o'erpast Sorry Second copy damn Mamillius killing        \",\n",
       "   'Hail pertinent box passages concluded staff fought walking wipe merely Affection         ']),\n",
       " (0.0001,\n",
       "  8,\n",
       "  0.5,\n",
       "  ['Her thorny ride   to : dregs :      :  he : : :',\n",
       "   \"high'st reasons condemn Dogs    :  : : that     :  not :\",\n",
       "   'lighted regreet entreat  : us   :  homicide    Dorset thy   he ',\n",
       "   'reputation AUMERLE wreak : priest-like dissolve :    :   :   ? : that :',\n",
       "   \"will't pipe Cupid   to  not    :    ?    :\",\n",
       "   'ague reply they :  :  :    :  : : to    ',\n",
       "   'repetition extremities had     :     :  :  ,   :',\n",
       "   'charges provoked Capel  mirth   :  priest-like   :   :   : :',\n",
       "   'fares John builds       :     Dogs magic  sting : :',\n",
       "   'day revoke not they : :      :  to   :  : ']),\n",
       " (0.0001,\n",
       "  8,\n",
       "  1.0,\n",
       "  ['Patrician unfit            throngs      ',\n",
       "   'cousin successful <UNK>                 ',\n",
       "   'gay thirst                  ',\n",
       "   'horrible tongues                  ',\n",
       "   'shut gainsay Worcester                 ',\n",
       "   'weary shake                  ',\n",
       "   'wild servants      ,            ',\n",
       "   'hum Have                  ',\n",
       "   'shift wing                  ',\n",
       "   'blade death                  ']),\n",
       " (0.0001,\n",
       "  8,\n",
       "  1.5,\n",
       "  ['adopted traitors                  ',\n",
       "   'knows complices                  ',\n",
       "   \"ta'en noted William                 \",\n",
       "   \"'Thou direction                  \",\n",
       "   'affect cloister haply                 ',\n",
       "   'solemnity busy                  ',\n",
       "   'surfeits infancy from                 ',\n",
       "   'muffled nothing army                 ',\n",
       "   'bustle consent circumstances                 ',\n",
       "   'fleet boldly hopeful                 ']),\n",
       " (0.0001,\n",
       "  16,\n",
       "  0.5,\n",
       "  [\"melted sacrament argues trumpet chaos Poor look treachery proudly Jule Adieu aboard drunken 't whiles I done Give Adam adverse\",\n",
       "   'ships reward greatness deer achieved soonest looked trees longer reputation purblind Leontes sinful shrewd helping lunatic Resolve fitter I posts',\n",
       "   'charitable goose mystery chastisement prerogative We token lace Who finger subdued BLUNT refer roof rites If resisted lunatic paying myself',\n",
       "   'partner religious courtship fare smooth venom chance lords issuing morsel drooping book calf commodity bands cousin stirr rapier fee sore',\n",
       "   'Princes glistering him thus lie occasion shunn conquered SICINIUS tears chin R Women duty heap assist Troth dislike ROMEO aqua',\n",
       "   'Father scarcely wanton flying favours posture craves father-in-law Behind Provost Jack not bashful lighted Exeter tell Let hold commodity up',\n",
       "   \"ice object with't returned 'scaped reigns I cheeks spurs suborn Deliver man puny daffodils Hostess import posterity oath Stanley tongueless\",\n",
       "   \"points ISABELLA fine inch Thither dies monument over-blown dukedom despair 'd impediment lechery mind Five descant Bristol successful Peace boots\",\n",
       "   'degrees parlous lain earl resign induced orator My desperate nursed credit you besiege town careless devilish sinful dealing worship Harry',\n",
       "   'discipline Black rebel Plays summers grievous bigger Save approve naked compounded comes eastern Get Apollo wreck promotions Clown ground beating']),\n",
       " (0.0001,\n",
       "  16,\n",
       "  1.0,\n",
       "  [\"tie until Fill going pearl arm blanks better anon swain pies ways kingdom anon reasonable tell'st , trumpets earl trumpets\",\n",
       "   \"provide nearer la remain blanks condemn left 's craves was Bearing swain scared eating beauteous craves coffers thumb courses trumpets\",\n",
       "   \"mortality vent god punished sigh flint afflict reasons ape pot him bestow him stain car coffers weep'st bury sky convented\",\n",
       "   \"different sore supper blame done Bearing , and shade , 'God convented flint alter Bearing maidenheads alter him buy thumb\",\n",
       "   \"Sorry decreed Came confessed buy weep'st shade , kingdom my , stain afflict out tell'st , scared him a a\",\n",
       "   'Lucy courts HORTENSIO reproof garter , part and , shade relish painting whom buy Rome whom pies Bearing him breath',\n",
       "   'tied divorce infect gilt done , roundly Bearing Bearing for panting afflict thumb craves bloody R follower for approach him',\n",
       "   \"fails mild urged danced injuries bosom pearl tell'st anon a ponderous exhales reasons courses knit pies exhales reasonable alter trumpets\",\n",
       "   \"glean destroy charitable usurer trumpets him reasons him , Mistake in 'Verily sly Full trumpets earl convented forsook Madam ,\",\n",
       "   \"'Zounds private An exhales whereon couple done my , cook argument child craves bury my strict retire , frank finish\"]),\n",
       " (0.0001,\n",
       "  16,\n",
       "  1.5,\n",
       "  ['wood protectors forlorn beasts beasts  gallant Justice I I import is bitterly thank bewray in imagine is I ',\n",
       "   \"stony wipe threes beside not Dismiss confident Sicilia 's , with , with with with disinherited bitterly Dismiss help imagine\",\n",
       "   'thrice evils crooked applause ; orange beasts strain codpiece conceive thank sight beasts garden-house I I posterity weeps is still',\n",
       "   \"slippery beaten Holy utters divide numbering defects unhallow still  cords 's be 's I orange , Dismiss denied \",\n",
       "   \"reproach securely brother 's cords defects I YORK ,  I conceive I ; I 's on To not this\",\n",
       "   'strains Spread I yield I HASTINGS is to-morrow   mutinies yield utters not it be not divide ; ,',\n",
       "   \"thinks awake 's cords of I I , unhallow intercession garden embrace  bewray pastime Barnet my I this utters\",\n",
       "   \"Dukes knew counted 's mind 's thereto not  ; gear shapes former YORK man not beasts , thou .\",\n",
       "   \"To-day mystery Ghost not defects when embrace not I determined  's with I 's confident still help beasts swiftly\",\n",
       "   \"proclaim privately Ragozine and is be embrace magistrates utters 's ; I I retail defects HASTINGS work my Intends is\"])]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shaky_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

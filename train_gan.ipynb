{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from encoding import EncodingManager, create_encoding_dataloader\n",
    "import encoding as encoding_module\n",
    "import utils\n",
    "from gan import Generator, Discriminator, train as train_gan\n",
    "import gan as gan_module\n",
    "import torch\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import ngram as ngram_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = 'data/raw_train.txt'\n",
    "LOG_DIR_BASE = './runs/'\n",
    "tokenized_sentences = utils.process_data(TRAIN_PATH)\n",
    "tokenized_sentences[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_lengths = [len(sentence) for sentence in tokenized_sentences]\n",
    "longest_sentence = max(tokenized_sentences, key=len)\n",
    "print(f'Longest sentence has {len(longest_sentence)} tokens')\n",
    "SEQ_LENGTH = len(longest_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraction_of_seq_lengths = {k: v / len(sentence_lengths) for k, v in Counter(sentence_lengths).items()}\n",
    "plt.bar(fraction_of_seq_lengths.keys(), fraction_of_seq_lengths.values(), label='Seq Length distribution')\n",
    "plt.xticks(np.arange(0, len(longest_sentence), 1))\n",
    "plt.xlabel('Sentence length')\n",
    "plt.savefig('./figs/seq_length_distribution.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding Dataloader checks\n",
    "\n",
    "Double check one hot encoding and decoding is working as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORD_ENCODING_MANAGER_PATH = 'data/word_to_index.json'\n",
    "word_encoding_manager = EncodingManager(WORD_ENCODING_MANAGER_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader = create_embedding_dataloader(tokenized_sentences, word2vec_manager, seq_length=SEQ_LENGTH, batch_size=4, encoding_method=\"one_hot\", verbose=True)\n",
    "# batch = next(iter(dataloader))\n",
    "# print(f'first batch sentence: {batch[0]}')\n",
    "# encoded_first_sentence = batch[0]\n",
    "# decoded_first_sentence = [word2vec_manager.decode_one_hot(encoded_token) for encoded_token in encoded_first_sentence]\n",
    "# decoded_first_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence_to_encode = ['That', 'you', 'have', \"ta'en\", 'a', 'tardy', '<UNK>', 'here', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
    "# encoded_sentence = [word2vec_manager.one_hot_encode(word) for word in sentence_to_encode]\n",
    "# decoded = [word2vec_manager.decode_one_hot(one_hot) for one_hot in encoded_sentence]\n",
    "# encoded_sentence,decoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "SEQ_LENGTH = 15\n",
    "seq_length = SEQ_LENGTH \n",
    "\n",
    "# Generator first\n",
    "\n",
    "gen_input_dim = len(word_encoding_manager.word_to_index) + 1 # add 1 to input dim to account for padding token\n",
    "gen_hidden_dim = 300\n",
    "\n",
    "# add 1 to output dim to account for padding token\n",
    "gen_output_dim = len(word_encoding_manager.word_to_index) + 1\n",
    "\n",
    "generator = Generator(input_size=gen_input_dim, hidden_size=gen_hidden_dim, output_size=gen_output_dim, seq_length=seq_length)\n",
    "\n",
    "\n",
    "# Discriminator\n",
    "# Discriminator input is the same as the generator output (the generated next token probability distribution)\n",
    "discrim_input_dim = gen_output_dim\n",
    "discrim_hidden_dim = 100\n",
    "\n",
    "discriminator = Discriminator(input_dim=discrim_input_dim, hidden_dim=discrim_hidden_dim, seq_length=seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discrim_params = list(discriminator.parameters())\n",
    "gen_params = list(generator.parameters())\n",
    "num_params_gen = sum([np.prod(p.size()) for p in gen_params])\n",
    "num_params_discrim = sum([np.prod(p.size()) for p in discrim_params])\n",
    "print(f'Generator has {num_params_gen} parameters')\n",
    "print(f'Discriminator has {num_params_discrim} parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train models\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.0001\n",
    "batch_size = 4\n",
    "temperature = 1.0\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "num_epochs = 1\n",
    "TRAIN_AND_VAL_FRAC = 1.0\n",
    "if TRAIN_AND_VAL_FRAC != 1.0:\n",
    "    train_val, rest = train_test_split(tokenized_sentences, train_size=TRAIN_AND_VAL_FRAC, random_state=42)\n",
    "    train_sents, val_sents = train_test_split(train_val, train_size=0.8, random_state=42)\n",
    "else:\n",
    "    train_sents, val_sents = train_test_split(tokenized_sentences, train_size=0.9, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentences(generator, temp):\n",
    "    gens = []\n",
    "    for i in range(10):\n",
    "        noise = torch.randn(1, seq_length, gen_input_dim)\n",
    "        generated_data = generator(noise, temperature, hard=False)\n",
    "        argmaxs = torch.argmax(generated_data[0], dim=1)\n",
    "        generated_sentence = [word2vec_manager.index_to_word(index) for index in argmaxs]\n",
    "        gens.append(\" \".join(generated_sentence).replace(\"<PAD>\", \"\"))\n",
    "    return gens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_hyperparams(possible_hyperparams, num_epochs, train_sents, val_sents, word_encoding_manager, metrics_dir, calc_perplexity):\n",
    "    assert len(possible_hyperparams.keys()) == 4 and set(possible_hyperparams.keys()) == set(['lr', 'temp', 'batch_size', 'seq_length'])\n",
    "    num_iterations = np.prod([len(possible_hyperparams[key]) for key in possible_hyperparams.keys()])\n",
    "    current_iteration = 0\n",
    "    results = []\n",
    "    for lr in possible_hyperparams['lr']:\n",
    "        for temp in possible_hyperparams['temp']:\n",
    "            for batch_size in possible_hyperparams['batch_size']:\n",
    "                for seq_length in possible_hyperparams['seq_length']:\n",
    "                    current_iteration += 1\n",
    "                    print(f'Iteration {current_iteration} of {num_iterations}')\n",
    "                    print(f'lr={lr}, temp={temp}, batch_size={batch_size}, seq_length={seq_length}')\n",
    "                    generator = Generator(input_size=gen_input_dim, hidden_size=gen_hidden_dim, output_size=gen_output_dim, seq_length=seq_length)\n",
    "                    discriminator = Discriminator(input_dim=discrim_input_dim, hidden_dim=discrim_hidden_dim, seq_length=seq_length)\n",
    "                    g_loss, d_loss =train_gan(\n",
    "                            generator=generator,\n",
    "                            discriminator=discriminator,\n",
    "                            generator_lr = lr,\n",
    "                            discriminator_lr = lr,\n",
    "                            validation_sentences=val_sents,\n",
    "                            training_sentences=train_sents,\n",
    "                            word_encoding_manager=word_encoding_manager,\n",
    "                            calc_perplexity=calc_perplexity,\n",
    "                            seq_length=SEQ_LENGTH,\n",
    "                            generator_input_features=gen_input_dim,  # Updated parameter name\n",
    "                            num_epochs=num_epochs,\n",
    "                            batch_size=batch_size,\n",
    "                            noise_sample_method=\"normal\",\n",
    "                            gumbel_hard=True,\n",
    "                            temperature=temp,\n",
    "                            device=device,\n",
    "                            tensorboard_log_dir=metrics_dir\n",
    "                        )\n",
    "                    gens = generate_sentences(generator, temp)\n",
    "                    results.append((lr, temp, batch_size, seq_length, g_loss, d_loss, generator, discriminator, gens))\n",
    "                    print(f'Generated sentences: {gens[:2]}')\n",
    "    return results\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_for_perplexity = ngram_module.NGRAM_Model(4, scoring_method='LI_grid_search')\n",
    "ngram_for_perplexity.train(train_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_perplexity = lambda sentences: gan_module.estimate_perplexity(sentences, ngram_for_perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "def prepare_metrics_dir(log_dir):\n",
    "    if not os.path.exists(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "    else:\n",
    "        # clear existing logs\n",
    "        for f in os.listdir(log_dir):\n",
    "            if os.path.isdir(os.path.join(log_dir, f)):\n",
    "                shutil.rmtree(os.path.join(log_dir, f))\n",
    "            else:\n",
    "                os.remove(os.path.join(log_dir, f))\n",
    "\n",
    "def run_experiment(possible_hyperparams, experiment_id, num_epochs, train_sents, val_sents, word_encoding_manager, calc_perplexity, log_dir_base):\n",
    "    log_dir = os.path.join(log_dir_base, \"hyperparameter_search_\" + experiment_id)\n",
    "    prepare_metrics_dir(log_dir)\n",
    "    results = test_hyperparams(possible_hyperparams, num_epochs, train_sents, val_sents, word_encoding_manager, log_dir, calc_perplexity)          \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_hyperparams = {\n",
    "        'lr': [0.0001],\n",
    "        'batch_size': [16],\n",
    "        'temp': [1.0],\n",
    "        'seq_length': [15],\n",
    "    }\n",
    "\n",
    "test_results = run_experiment(possible_hyperparams, \"test\", num_epochs, train_sents, val_sents, word_encoding_manager, calc_perplexity, LOG_DIR_BASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False # don't run this cell again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_BATCH_MODEL_PATH = './models/generator_batch_size_4.pt'\n",
    "if os.path.exists(BEST_BATCH_MODEL_PATH):\n",
    "    generator = gan_module.load_gen_model(BEST_BATCH_MODEL_PATH, gen_input_dim, gen_hidden_dim, gen_output_dim, seq_length)\n",
    "    batch_size_results = [(0.001, 1.0, 4, 15, 0.0, 0.0, generator, None, generate_sentences(generator, 1.0))]\n",
    "else:\n",
    "    possible_hyperparams = {\n",
    "        'lr': [0.001],\n",
    "        'batch_size': [4, 16, 32],\n",
    "        'temp': [1.0],\n",
    "        'seq_length': [15],\n",
    "    }\n",
    "    batch_size_results = run_experiment(possible_hyperparams, \"batch_size\", num_epochs, train_sents, val_sents, word_encoding_manager, calc_perplexity, LOG_DIR_BASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_SEQ_LENGTH_MODEL_PATH = './models/generator_seq_length_13.pt'\n",
    "if os.path.exists(BEST_SEQ_LENGTH_MODEL_PATH):\n",
    "    generator = gan_module.load_gen_model(BEST_SEQ_LENGTH_MODEL_PATH, gen_input_dim, gen_hidden_dim, gen_output_dim, seq_length)\n",
    "    seq_length_results = [(0.001, 1.0, 4, 13, 0.0, 0.0, generator, None, generate_sentences(generator, 1.0))]\n",
    "else:\n",
    "    possible_hyperparams = {\n",
    "        'lr': [0.001],\n",
    "        'batch_size': [16],\n",
    "        'temp': [1.0],\n",
    "        'seq_length': [13,15],\n",
    "    }\n",
    "    seq_length_results = run_experiment(possible_hyperparams, \"seq_length\", num_epochs, train_sents, val_sents, word_encoding_manager, calc_perplexity, LOG_DIR_BASE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_TEMP_MODEL_PATH = './models/generator_temp_1.0.pt'\n",
    "if os.path.exists(BEST_TEMP_MODEL_PATH):\n",
    "    generator = gan_module.load_gen_model(BEST_TEMP_MODEL_PATH, gen_input_dim, gen_hidden_dim, gen_output_dim, seq_length)\n",
    "    temp_results = [(0.001, 1.0, 4, 15, 0.0, 0.0, generator, None, generate_sentences(generator, 1.0))]\n",
    "else:\n",
    "    possible_hyperparams = {\n",
    "        'lr': [0.001],\n",
    "        'batch_size': [16],\n",
    "        'temp': [1.0, 1.5],\n",
    "        'seq_length': [15],\n",
    "    }\n",
    "\n",
    "    temp_results = run_experiment(possible_hyperparams, \"temp\", num_epochs, train_sents, val_sents, word_encoding_manager, calc_perplexity, LOG_DIR_BASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_LR_MODEL_PATH = './models/generator_lr_0.0001.pt'\n",
    "if os.path.exists(BEST_LR_MODEL_PATH):\n",
    "    generator = gan_module.load_gen_model(BEST_LR_MODEL_PATH, gen_input_dim, gen_hidden_dim, gen_output_dim, seq_length)\n",
    "    lr_results = [(0.0001, 1.0, 16, 15, 0.0, 0.0, generator, None, generate_sentences(generator, 1.0))]\n",
    "else:\n",
    "    possible_hyperparams = {\n",
    "        'lr': [0.0001],\n",
    "        'batch_size': [16],\n",
    "        'temp': [1.0],\n",
    "        'seq_length': [15],\n",
    "    }\n",
    "\n",
    "    lr_results = run_experiment(possible_hyperparams, \"lr\", num_epochs, train_sents, val_sents, word_encoding_manager, calc_perplexity, LOG_DIR_BASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(data, window_size):\n",
    "    return np.convolve(data, np.ones(window_size)/window_size, mode='valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DISCRIM_LOSS_CSV_PATH = 'metrics/Loss _ Train_Discriminator Loss.csv'\n",
    "GEN_LOSS_CSV_PATH = 'metrics/Loss _ Train_Generator Loss.csv'\n",
    "GAN_LOSS_PLOT_PATH = 'figs/gan_loss_plot.png'\n",
    "\n",
    "import pandas as pd\n",
    "discrim_loss_df = pd.read_csv(DISCRIM_LOSS_CSV_PATH)\n",
    "gen_loss_df = pd.read_csv(GEN_LOSS_CSV_PATH)\n",
    "\n",
    "window_size = 40\n",
    "\n",
    "smoothed_discrim_loss = moving_average(discrim_loss_df['Value'], window_size)\n",
    "smoothed_gen_loss = moving_average(gen_loss_df['Value'], window_size)\n",
    "\n",
    "\n",
    "plt.plot(discrim_loss_df['Step'], discrim_loss_df['Value'], alpha=0.2, c='#e52592')\n",
    "plt.plot(gen_loss_df['Step'], gen_loss_df['Value'], alpha=0.2, c='#12b5cb')\n",
    "plt.plot(discrim_loss_df['Step'][:-(window_size - 1)], smoothed_discrim_loss, label='Discriminator loss', c='#e52592')\n",
    "plt.plot(gen_loss_df['Step'][:-(window_size - 1)], smoothed_gen_loss, label='Generator loss', c='#12b5cb')\n",
    "plt.xlabel('Batch #')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('GAN Loss')\n",
    "# log scale\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig(GAN_LOSS_PLOT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now plot perplexity\n",
    "PERPLEXITY_CSV_PATH = 'metrics/perplexity.csv'\n",
    "perplexity_df = pd.read_csv(PERPLEXITY_CSV_PATH)\n",
    "plt.plot(perplexity_df['Step'], perplexity_df['Value'], alpha=0.9, c='#e52592')\n",
    "plt.xlabel('Batch #')\n",
    "plt.ylabel('Mean Perplexity')\n",
    "#log scale\n",
    "plt.title('GAN Perplexity')\n",
    "plt.yscale('log')\n",
    "plt.savefig('figs/gan_perplexity_plot1.png')\n",
    "# log scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " possible_hyperparams = {\n",
    "        'lr': [0.0001],\n",
    "        'batch_size': [16],\n",
    "        'temp': [1.0],\n",
    "        'seq_length': [15],\n",
    "    }\n",
    "\n",
    "lr_results = run_experiment(possible_hyperparams, \"lr\", num_epochs, train_sents, val_sents, word2vec_manager, calc_perplexity, LOG_DIR_BASE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shaky_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
